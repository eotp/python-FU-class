{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Inferential statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the `src` directory as one where we can import modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# add the 'src' directory as one where we can import modules\n",
    "src_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'src'))\n",
    "sys.path.append(src_dir)\n",
    "print(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_funcs as hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population and Sample Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Inferential statistics](https://en.wikipedia.org/wiki/Statistical_inference) is all about using **sample results** to make decisions or predictions about a **population**. Basically, a numerical value is assigned to a population parameter based on the information collected from a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = hf.sample_body_heights(20)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(x, bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge: Repeat the experiment above but this time take 20,000 samples. Save the results in the variable `heights` \n",
    "> * ### Compute and report the mean and the standard deviation of the sample. Store them in variables denoted as `xbar` and `std`, respectively.  \n",
    "> * ### Plot a histogram. Describe characteristic features of the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/height_experiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization (standard (z)-scores)\n",
    "\n",
    "In statistics, the standard score is the signed fractional number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured. Observed values above the mean have positive standard scores, while values below the mean have negative standard scores.\n",
    "\n",
    "It is calculated by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation.  Standard scores are also called z-values, z-scores, normal scores, and standardized variables.\n",
    "\n",
    "$$z = \\frac{X-\\bar x}{s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge: Write a function `ztrans` and implement the logic as stated above. Apply the function to the variable `heights` and plot a historgram. What changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/ztrans.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal distribution\n",
    "\n",
    "The [**normal distribution**](https://en.wikipedia.org/wiki/Normal_distribution) is used extensively in probability theory, statistics, and the natural and social sciences. It is also called the **Gaussian distribution** because [Carl Friedrich Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss) (1777-1855) was one of the first to apply it for the analysis of astronomical data.\n",
    "\n",
    "The **normal probability distribution** or the **normal curve** is a bell-shaped (symmetric) curve. Its mean is denoted by $\\mu$ and its standard deviation by $\\sigma$. A continuous random variable $x$ that has a normal distribution is called a **normal random variable**. \n",
    "\n",
    "The notation for a normal distribution is $X \\sim N(\\mu,\\sigma)$. The probability density function (PDF) is written as \n",
    "\n",
    "$$f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}.$$\n",
    "\n",
    "The probability density function $f(x)$ gives the vertical distance between the horizontal axis and the normal curve at point $x$. \n",
    "\n",
    "The normal distribution is described by two parameters, the mean, $\\mu$, and the standard deviation, $\\sigma$. Each different set of values of $\\mu$ and $\\sigma$ gives a different normal distribution. The value of $\\mu$ determines the center of a normal distribution curve on the horizontal axis, and the value of $\\sigma$ gives the spread of the normal distribution curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0 \n",
    "sigma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-4, 4, 100)\n",
    "y = norm.pdf(x, loc=mu, scale=sigma)\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normal distribution is characterized, among others, by the following characteristics:\n",
    "\n",
    "1. The total area under a normal distribution curve is 1.0, or 100%.  \n",
    "2. A normal distribution curve is symmetric about the mean. Consequently, 50% of the total area under a normal distribution curve lies on the left side of the mean, and 50% lies on the right side of the mean.  \n",
    "3. The tails of a normal distribution curve extend indefinitely in both directions without touching\n",
    "or crossing the horizontal axis. Although a normal distribution curve never meets the horizontal axis, beyond the points represented by $\\mu - 3\\sigma$ and $\\mu + 3\\sigma$ it becomes so close to this axis that the area under the curve beyond these points in both directions can be taken as virtually zero.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding $z_\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.ppf(0.95, loc=mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the area to the left of a specified $z$-score or $x$-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.cdf(z, loc=mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the area to the right of a specified $z$-score or $x$-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - norm.cdf(z, loc=mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the area inbetween two specified $z$-scores or $x$-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.cdf(z, loc=mu, scale=sigma) - (1 - norm.cdf(z, loc=mu, scale=sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.visualize_probabilities(p=0.25,loc=mu, scale=sigma, tails='both', figsize=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge: Play around with the function `hf.visualize_probabilities` and try to reason about it. Note that you can pick among different types of tails (`lower`, `upper`, `both`, and `inbetween`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/eval_visualization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge: Confirm the [Empirical Rule](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule), also known as the **68-95-99.7 rule**, which relates to [Chebyshev's theorem](https://en.wikipedia.org/wiki/Chebyshev%27s_theorem).\n",
    "For a bell-shaped distribution the 3 rules are, that approximately\n",
    "> * 68% of the observations lie within one standard deviation of the mean,\n",
    "> * 95% of the observations lie within two standard deviations of the mean, and\n",
    "> * 99.7% of the observations lie within three standard deviations of the mean.\n",
    "> ### Compute three varialbles (`auc1`,`auc2`, and `auc3`), each ot them area under the curve for the intervals $[\\mu - 1 \\times \\sigma, \\mu + 1 \\times \\sigma]$, $[\\mu - 2 \\times \\sigma, \\mu + 2 \\times \\sigma]$ and $[\\mu - 3 \\times \\sigma, \\mu + 3 \\times \\sigma]$.  \n",
    "_Hint: Use the `norm.cdf` function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0\n",
    "sigma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here (replace None)\n",
    "auc1 = None\n",
    "auc2 = None\n",
    "auc3 = None\n",
    "print(auc1, auc2, auc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/auc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,11), nrows=3)\n",
    "for e, p in enumerate((auc1, auc2, auc3)):\n",
    "    hf.visualize_probabilities(p=p, loc=mu, scale=sigma, tails='inbetween', ax=ax[e])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Central Limit Theorem\n",
    "\n",
    "The [__central limit theorem__](https://en.wikipedia.org/wiki/Central_limit_theorem) is one of the most useful concepts in statistics. The theorem is all about drawing samples from a population. The theorem states that if one collects samples of a large enough sample size $n$, and calculates each sample’s mean (or sum, etc.), the shape of the histogram of those statistics approximates a bell shaped (normal) form. The usefulness of the central limit theorem is due to the fact, that it does not matter what the distribution of the original population is, the distribution of sample statistics follows the normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling error\n",
    "\n",
    "Obviously, different samples (of the same length) selected from the same population yield different sample statistics because they contain different elements. Moreover, any sample statistics obtained from any sample, such as the sample mean $\\bar x$, will be different from the result obtained from the corresponding population, the population mean, $\\mu$. The difference between the value of a sample statistic obtained from a sample and the value of the corresponding population parameter obtained from the population is called the **[sampling error](https://en.wikipedia.org/wiki/Sampling_error)**. In the case of the mean the sampling error can be written as \n",
    "\n",
    "$$\\text{sampling error} = \\bar x - \\mu$$\n",
    "\n",
    "Due to the nature of random sampling, and thus due to the process of drawing a set of values from the population, the resulting sampling error occurs due to chance, or in other words, the sampling error is a random variable. However, one should note that beside the described randomness there are other sources of error. These error are often related the the data generation process and are subsumed under the term [non-sampling error](https://en.wikipedia.org/wiki/Non-sampling_error). Such errors are introduced by for example human handling of the data, calibration errors of the measuring devices, among others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, mu, sigma = hf.sample_body_heights(return_params=True)\n",
    "print(mu)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.rvs(loc=mu, scale=sigma, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling error\n",
    "norm.rvs(loc=mu, scale=sigma, size=1) - mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge: Compute and report the sampling error for random samples of sample sizes of 3, 30, 300 and 3000 (keep $\\mu$ and $\\sigma$ the same as before). \n",
    "_Hint: You may write a for-loop_\n",
    "\n",
    ">_Reminder: $$\\text{sampling error} = \\bar x - \\mu$$_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/sample_error.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sampling distribution\n",
    "\n",
    "Based upon our intuition of randomness in the sampling process, we introduce the **[Sampling Distribution](https://en.wikipedia.org/wiki/Sampling_distribution)**. The sampling distribution is a distribution of a sample statistic. Often the name of the computed statistic is added as part of the title. For example, if the computed statistic was the sample mean, the sampling distribution would be titled **the sampling distribution of the sample mean**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The more often we take a sample the better the relative frequency distribution of the sample statistics approximates the sampling distribution. Or in other words, as the number of samples approaches infinity, the resulting frequency distribution will approach the sampling distribution. The sampling distribution should not be confused with a sample distribution: the latter describes the distribution of values (elements) in one particular sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data comming from a normally distributed random variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25\n",
    "mu = 100\n",
    "sigma = 13\n",
    "samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_xs = []\n",
    "for sample in range(samples):\n",
    "    x = norm.rvs(loc=mu, scale=sigma, size=n)\n",
    "    xbar = np.mean(x)\n",
    "    mu_xs.append(xbar)\n",
    "\n",
    "print(f'True mu: {mu}, mean of the sampling distribution: {np.mean(mu_xs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(mu_xs, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data comming from a not normally distribited random variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "fig, ax = plt.subplots()\n",
    "lambda_ = 17\n",
    "ax.hist(expon.rvs(size=1000, loc=0, scale=lambda_));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25\n",
    "samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_xs = []\n",
    "for sample in range(samples):\n",
    "    x = expon.rvs(size=n, loc=0, scale=lambda_)\n",
    "    xbar = np.mean(x)\n",
    "    mu_xs.append(xbar)\n",
    "\n",
    "print(f'True lambda: {lambda_}, mean of the sampling distribution: {np.mean(mu_xs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(mu_xs, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge: Experiment with differnt sample sizes `n`. At which sample size does the sampling distribution approximate a normal distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The standard error\n",
    "\n",
    "Just as the population distributions can be described with parameters, so can the sampling distribution. The expected value (mean) of any distribution can be represented by the symbol $\\mu$ (mu). In the case of the sampling distribution, the mean, $\\mu$, is often written with a subscript to indicate which sampling distribution is being described. For example, the expected value of the sampling distribution of the mean is represented by the symbol $\\mu_{\\bar x}$. The value of $\\mu_{\\bar x}$ can be thought of as the theoretical mean of the distribution of sample means.\n",
    "\n",
    "If we pick a large enough number samples (of the same size) from a population and calculate their means, then the mean ($\\mu_{\\bar x}$ ) of all these sample means will approximate the mean ($\\mu$) of the population. That is why the the sample mean $\\bar x$, is called an estimator of the population mean, $\\mu$. Thus, the mean of the sampling distribution is equal to the mean of the population. \n",
    "\n",
    "$$\\mu_{\\bar x} = \\mu$$\n",
    "\n",
    "\n",
    "The standard deviation of a sampling distribution is given a special name, the [**standard error**](https://en.wikipedia.org/wiki/Standard_error). The standard error of the sampling distribution of a statistic, denoted as $\\sigma_{\\bar x}$, describes the degree to which the computed statistics may be expected to differ from one another when calculated from a sample of similar size and selected from similar population models. The larger the standard error of a given statistic, the greater the differences between the computed statistics for the different samples (Lovric 2010). \n",
    "\n",
    "However, please note that the standard error, $\\sigma_{\\bar x}$, is not equal to the standard deviation, $\\sigma$, of the population distribution (unless $n = 1$). The standard error is equal to the standard deviation of the population divided by the square root of the sample size:\n",
    "\n",
    "\n",
    "$$ \\sigma_{\\bar x} = \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "\n",
    "This equation holds true only when the sampling is done either with replacement from a finite population or with or without replacement from an infinite population. Which corresponds to the condition that the sample size $(n)$ is small in comparison to the population size $(N)$. The sample size is considered to be small compared to the population size if the sample size is equal to or less than 5% of the population size that is, if\n",
    "\n",
    "$$ \\frac{n}{N} \\le 0.05$$\n",
    "\n",
    "If this condition is not satisfied, the following equation is used to calculate $\\sigma_{\\bar x}$:\n",
    "\n",
    "$$ \\sigma_{\\bar x} = \\frac{\\sigma}{\\sqrt{n}}\\sqrt{\\frac{N-n}{N-1} }$$\n",
    "\n",
    "In most practical applications, however, the sample size is small compared to the population size. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_error(mu_xs, n):\n",
    "    sigma = np.std(mu_xs)\n",
    "    return sigma/np.sqrt(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_error(mu_xs=mu_xs, n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Estimate and Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a sample, the value of the computed sample statistic gives a point estimate of the corresponding population parameter. For example, the sample mean $(\\bar x)$, is a point estimate of the corresponding population mean, $\\mu$, or the sample standard deviation $s$ is a point estimate for the population standard deviation $\\sigma$. \n",
    "\n",
    "* [__sampling error__](https://en.wikipedia.org/wiki/Sampling_error) (the point estimate almost always differs from the true value of the population)\n",
    "\n",
    "\n",
    "### Interval Estimate \n",
    "\n",
    "Instead of assigning a single value to a population parameter, an interval estimation gives a probabilistic statement, relating the given interval to the probability that this interval actually contains the true (unknown) population parameter.\n",
    "\n",
    "\n",
    "The level of confidence is chosen a priori and thus depends on the users preferences. It is denoted by\n",
    "\n",
    "$$100(1-\\alpha)$$\n",
    "\n",
    "Although any value of confidence level can be chosen, the most common values are 90%, 95%, and 99%. When expressed as probability, the confidence level is called the confidence coefficient and is denoted by (1−$\\alpha$). Most common confidence coefficients are 0.90, 0.95, and 0.99, respectively.\n",
    "\n",
    "A 100(1−$\\alpha$)% confidence interval is an interval estimate around a population parameter $\\theta$ (here, the Greek letter $\\theta$ is a placeholder for any population parameter of interest such as the mean $\\mu$, or the standard deviation $\\sigma$, among others) that, under repeated random samples of size $N$, is expected to include $\\theta$'s true value 100(1−$\\alpha$)%  of the time.\n",
    "\n",
    "The actual number added to and subtracted from the point estimate is called the margin of error.\n",
    "\n",
    "$$CI:\\text{Point estimate} \\pm \\text{Margin of error (ME)}$$\n",
    "\n",
    "\n",
    "Thus, the margin of error (ME) is expressed as\n",
    "\n",
    "\n",
    "$$ME = t^*_{\\text{df;}\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence interval the hard way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedure\n",
    "##### 1. Compute the sample mean (`xbar`)\n",
    "##### 2. Compute the t-score (`t_`) for $\\alpha = 0.05$ and $df=n-1$\n",
    "##### 3. As $\\sigma$ is unknown compute the standard devitation of the sample `s`\n",
    "##### 4. Compute the margin of error (`me`) as $ME = t^*_{\\text{df;}\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}}$\n",
    "##### 5. Compute the lower and upper 95% confidence interval as $CI:\\text{Point estimate} \\pm \\text{Margin of error (ME)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data set\n",
    "\n",
    "Source [Crowder, M. and Hand, D. (1990)](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/ChickWeight.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken, chick_diet1, chick_diet2, chick_diet3, chick_diet4 = hf.load_chicken()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken[\"weight\"].plot.hist(figsize=(13,5), bins=15)\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Compute the sample mean (`xbar`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/ci_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compute the t-score (`t_`) for $\\alpha = 0.05$ and $df=n-1$ \n",
    "_Hint: use the `t.ppf` function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/ci_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. As $\\sigma$ is unknown compute the standard devitation of the sample `s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/ci_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Compute the margin of error (`me`) as \n",
    ">$ME = t^*_{\\text{df;}\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/ci_4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Compute the lower and upper 95% confidence interval as $CI:\\text{Point estimate} \\pm \\text{Margin of error (ME)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/ci_5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence intervals the easy way using `statsmodels`\n",
    "\n",
    "[statsmodels](http://www.statsmodels.org/dev/index.html#) is a Python module that provides classes and functions for the estimation of many different **statistical models**, as well as for conducting **statistical tests**, and **statistical data exploration**.\n",
    "\n",
    "\n",
    "* Regression Analysis\n",
    "* Linear Mixed Effects Models\n",
    "* ANOVA\n",
    "* Time Series analysis\n",
    "* Parametric and Nonparametric Statistical Methods\n",
    "* Multivariate Statistics multivariate\n",
    "* Distributions\n",
    "* and many more...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "d_stats = DescrStatsW(chicken[\"weight\"])\n",
    "type(d_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_mean = d_stats.mean\n",
    "d_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "d_conf_int = d_stats.tconfint_mean(alpha=alpha) \n",
    "print(d_conf_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very common problem that scientists face is the assessment of significance in scattered statistical data. Owing to the limited availability of observational data, scientists apply **inferential statistical methods to decide if the observed data contains significant information or if the scattered data is nothing more than the manifestation of the inherently probabilistic nature of the data generation process**.\n",
    "\n",
    "The framework of hypothesis testing is all about making statistical inferences about populations based on samples taken from the population. Any hypothesis test involves the **collection of data (sampling)**. If the **hypothesis** is assumed to be correct, the scientist can calculate the **expected results** of an experiment. If the **observed data** differs significantly from the expected results, then one considers the assumption to be incorrect. Thus, based on the observed data the scientist makes a **decision** as to whether or not there is sufficient evidence, based upon analyses of the data, that the model - the hypothesis - should be rejected, or that there is not sufficient evidence to reject the stated hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(13,6))\n",
    "ax = axes.ravel()\n",
    "for e, diet in enumerate(chicken.Diet.unique()):\n",
    "    chicken.loc[chicken.Diet==diet,\"weight\"].plot.hist(ax=ax[e])\n",
    "    ax[e].grid()\n",
    "    ax[e].set_title(\"Diet {}\".format(diet))\n",
    "    ax[e].set_xlim(30, 200)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken.groupby(\"Diet\")[\"weight\"].mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ci(df, group, var, alpha=0.05):\n",
    "    groups = df[group].unique()\n",
    "    rv = pd.DataFrame({\"group\":None, \n",
    "                       \"mean\":None,\n",
    "                       \"lower_ci\":None,\n",
    "                       \"upper_ci\":None}, \n",
    "                      index=range(len(groups)))\n",
    "    for e, g in enumerate(groups):        \n",
    "        stats = DescrStatsW(df.loc[df[group] == g, var])\n",
    "        group_mean = stats.mean\n",
    "        group_ci = stats.tconfint_mean(alpha=alpha) \n",
    "        rv.loc[e] = {\"group\":g, \n",
    "                     \"mean\":group_mean,\n",
    "                     \"lower_ci\":group_ci[0],\n",
    "                     \"upper_ci\":group_ci[1]}\n",
    "    return rv\n",
    "        \n",
    "group_stats = compute_ci(df=chicken, group=\"Diet\", var=\"weight\")    \n",
    "group_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = group_stats[\"group\"].values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(group_stats[\"mean\"].values, groups, \"o\")\n",
    "ax.set_ylim(0.5,4.5)\n",
    "\n",
    "for e,i in enumerate(group_stats[\"group\"]):\n",
    "    mean = group_stats[\"mean\"].values[e]\n",
    "    upper = group_stats[\"upper_ci\"].values[e]\n",
    "    lower = group_stats[\"lower_ci\"].values[e]\n",
    "    ax.plot((upper, lower),\n",
    "            (i,i), \"k--\")\n",
    "    ax.text(mean,i+0.1, \"Diet \"+str(int(i)), horizontalalignment='center')\n",
    "ax.legend([\"mean\", \"ci interval\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for differences in the mean (t-test)\n",
    "\n",
    "* variables normally distributed and independent\n",
    "* variance equal or not\n",
    "\n",
    "Null hypothesis\n",
    "$H_0:\\quad \\mu_1 = \\mu_2$\n",
    "\n",
    "Alternative hypothesis\n",
    "$H_A:\\quad \\mu_1 \\ne \\mu_2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.weightstats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, p, degf = ttest_ind(chick_diet4, chick_diet1)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge: Try out all combinations of `chick_diet1`, `chick_diet2`, `chick_diet3` and `chick_diet4` and report the $p$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/chicken_pval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with multiple comparisons is that the more hypotheses are tested on a particular data set, the more likely it is to incorrectly reject the null hypothesis. Thus, methods of multiple comparison require a higher significance threshold ($\\alpha$) for individual comparisons, in order to compensate for the number of inferences being made.\n",
    "\n",
    "\n",
    "**The Family-wise error rate**\n",
    "\n",
    "The family-wise error rate is the probability of making one or more false discoveries, or [__Type I errors__](https://2378nh2nfow32gm3mb25krmuyy-wpengine.netdna-ssl.com/wp-content/uploads/2014/05/Type-I-and-II-errors1-625x468.jpg) when performing multiple hypotheses tests.\n",
    "\n",
    "Recall that at a significance level of $\\alpha=0.05$, the probability of making a Type I error is equal to $0.05$ or $5\\%$. Consequently, the probability of not making a Type I error is $1-\\alpha=1-0.05=0.95$.\n",
    "\n",
    "\n",
    "Written more formally, for a family of $C$ tests, the probability of making a Type I error for the whole family is\n",
    "\n",
    "$$(1-\\alpha)^C\\text{.}$$\n",
    "\n",
    "Let us now consider $C=4$ and $\\alpha=0.05$.\n",
    "\n",
    "Thus, if we make 4 multiple comparisons on one data set, the probability of not making one or more Type I errors on the family of tests is $(1-\\alpha)^C = (1−0.05)^4 = 0.81$\n",
    "\n",
    "\n",
    "Null hypothesis\n",
    "$H_0:\\quad \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4$\n",
    "\n",
    "Alternative hypothesis\n",
    "$H_A:\\quad \\mu_1 \\ne \\mu_2 \\ne \\mu_3 \\ne \\mu_4$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Tukey's HSD (Honestly Significant Difference)](https://en.wikipedia.org/wiki/Tukey%27s_range_test)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.sandbox.stats.multicomp import MultiComparison\n",
    "mult_comp = MultiComparison(data=chicken[\"weight\"], groups=chicken[\"Diet\"])\n",
    "mult_comp.tukeyhsd(alpha=0.05).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = mult_comp.tukeyhsd(alpha=0.05).plot_simultaneous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
