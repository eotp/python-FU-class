{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with [Python](https://www.python.org/) using [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) and [`requests`](https://2.python-requests.org/en/master/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to scarpe the content of ABV training courses from the [Vorlesungsverzeichnis](https://www.fu-berlin.de/vv/de/modul?id=478016&sm=498562) and analyze its content by generating a wordcloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Set up__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importieren von Python Bibliotheken__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please check first the [https://www.fu-berlin.de/robots.txt](https://www.fu-berlin.de/robots.txt) site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.fu-berlin.de'\n",
    "abv = '/vv/de/modul?id=478016&sm=498562'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit the website above an try to figure out where the data of interest, the review texts, is made available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the content of the website using `requests` and  `BeautifulSoup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(url+abv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge 1: Inspect the `soup` object and try to make sense of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/soup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the revelant item where the data is made available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge 2: Extract data for the course name and the internet link to that course where additional information may be found  \n",
    "> * #### Inspect the `soup` object or visit the website to find out where the data we are looking is made available. \n",
    "> * #### [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) offers many methods and attribiutes to extract data from an html file. Particular useful are the `find` and the `find_all` methods.\n",
    "> * #### Build a pandas dataframe denoted as `data` with all course names and internet links in two columns denoted as `course_names` and `course_links`.\n",
    "> * #### How many courses are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/build_dataframe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge 3: Follow the link (`link`) as given below and exract the text data corresponing to the information on the website to the given link. \n",
    "> * #### Use the `requests` and `beautifulsoup` modules to get the job done. \n",
    "> * #### Write a function called `text_exctraction`, taking only one argument, the internet link. The output should be a list of (not yet cleaned) strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://www.fu-berlin.de/vv/de/lv/542611?m=348436&pc=478016&sm=498562'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/text_exctraction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = text_exctraction(link)\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge 4: Write a function `clean_text` that cleans the text data. \n",
    "> * #### Make sure you account for `\\n`, `\\r` and whitespaces.\n",
    "> * #### You may also consider to dump the word `SchlieÃŸen` and exlcude a text block if it startswith `Anmeldung`.\n",
    "> * #### _Note: The input of the function will be a list of strings!_\n",
    "\n",
    "\n",
    "#### The result should look like this:\n",
    "\n",
    "    'This ABV English module is designed to enable students to better cope with the challenges of using English within a higher education context. Throughout this introductory module, we will look at learning strategies and study skills to help students to develop English for study purposes in the four key language skills: reading, listening, speaking and writing. Students will be expected to produce a portfolio of written work throughout the course and to deliver a short study-related presentation.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/clean_text.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the computer do the work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge 5: Write a function `extract_comments` that takes in links and returns the extracted text from all links. \n",
    "> * #### Make sure your function has an argument the provides the number of links to be followed.\n",
    "> * #### Also try to implement the `time.sleep` function with random sleeping time. This does make it more likely that your IP is not flagged ;-)\n",
    "\n",
    ">    `from numpy import random`   \n",
    ">    `import time`   \n",
    ">    `r = random.random()`   \n",
    ">    `time.sleep(r)`\n",
    "> * #### Consider reusing the functions `text_exctraction` and `clean_text` from above. \n",
    "> * #### Make sure your functions returns one string and no duplicates. Functions such as the `set` function ot the `\" \".join` function may be handy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/extract_comments.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_comments(links=df.course_links, n_links=7, rs=23, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the wordcloud using stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('de') + get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(stopwords=stop_words).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a really fancy wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "mask = np.array(Image.open(\"../data/images/berlin_bear.png\"))   #choose mask\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stop_words,\n",
    "    background_color=\"white\",\n",
    "                    mask=mask,\n",
    "                    mode=\"RGB\",\n",
    "                    random_state=42\n",
    "                    ).generate(text)\n",
    "\n",
    "\n",
    "# Display the generated image:\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge 6: Improve the wordcloud as you whish. \n",
    "> * #### Therfore you may play around with any arguments of the `wordcloud` function.\n",
    "> * #### Feel free to add any other mask of your choice.\n",
    "> * #### Add or remove stopwords as you like.\n",
    "> * #### In order to have more fun the full data set is provided to you. Uncomment the cell below to access the extracted text from all currently available courses of ABV (`full_text`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_text = pickle.load(open('../data/full_text.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
