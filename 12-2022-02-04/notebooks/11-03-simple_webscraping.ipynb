{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with [Python](https://www.python.org/) using [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) and [`requests`](https://2.python-requests.org/en/master/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to scarpe the content of ABV training courses from the [Vorlesungsverzeichnis](https://www.fu-berlin.de/vv/de/modul?id=478016&sm=606239) and analyze its content by generating a wordcloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Set up__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import Libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please check first the [https://www.fu-berlin.de/robots.txt](https://www.fu-berlin.de/robots.txt) site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.fu-berlin.de'\n",
    "abv = '/vv/de/modul?id=478016&sm=606239'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit the website `https://www.fu-berlin.de/vv/de/modul?id=478016&sm=606239` an try to figure out where the data of interest, the review texts, is made available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the content of the website using `requests` and  `BeautifulSoup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url+abv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(url+abv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge 1: Inspect the `soup` object and try to make sense of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/soup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the revelant item where the data is made available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge 2: Extract data for the course name and the internet link to the course where additional information may be found  \n",
    "> * #### Inspect the `soup` object or visit the website to find out where the data is made available. \n",
    "> * #### [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) offers many methods and attributes to extract data from a html file. Particular useful are the `find` and the `find_all` methods.\n",
    "> * #### Build a pandas dataframe denoted as `data` with all course names and internet links in two columns denoted as `course_names` and `course_links`.\n",
    "> * #### How many courses are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/build_dataframe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge 3: Follow the link (`link`) as given below and extract the text data corresponding to the information on the website to the given link. \n",
    "> * #### Use the `requests` and `beautifulsoup` modules to get the job done. \n",
    "> * #### Write a function called `text_extraction`, taking only one argument, the internet link. The output should be a list of (not yet cleaned) strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://www.fu-berlin.de/vv/de/lv/658261?m=348815&pc=478016&sm=606239'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/text_extraction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = text_extraction(link)\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge 4: Write a function `clean_text` that cleans the text data. \n",
    "> * #### Make sure you account for `\\n`, `\\r` and whitespaces.\n",
    "> * #### You may also consider to dump the word `Schließen`\n",
    "> * #### _Note: The interesting data is hidden in the second item of the list_\n",
    "> * #### _Note: You may consider using the string method `replace(\n",
    "\n",
    "\n",
    "#### The result should look like this:\n",
    "\n",
    "    'Modul E - Lehrveranstaltung 2  Hinweis:   Vorkenntnisse sind nicht erforderlich.  Inhalte des Moduls   Datenanalyse mit Python   Python ist eine einfach zu erlernende Programmiersprache, die sich für viele wissenschaftliche Anwendungen hervorragend eignet. In diesem Kurs erlernen Sie die Grundlagen von Python. Das Schreiben einfacher Programme, die nützliche Aufgaben übernehmen können, ist dabei die erste Aufgabe. Sie werden lernen, Python als Datenanalysen und -visualisierungstool zu nutzen, um komplexe Aufgabenstellungen zu meistern. Die zusammenfassende Darstellung von Datenanalysen werden Sie visuell ansprechend erstellen lernen. Vorkenntnisse sind nicht erforderlich.   Programmierung mit Python I   In den ersten Kurswochen lernen Sie die grundlegende Befehlssyntax von Python kennen. Die eingebauten Datenstrukturen werden dabei Schritt für Schritt abgehandelt, ebenso wie die Ein- und Ausgabe von Dateien. Über Schleifen und Verzweigungen lernen Sie, die Ausführung Ihres Programms zu steuern. Schließlich werden Sie Funktionen aus der Python-Standardbibliothek und anderen open-source Hilfsbibliotheken anwenden, um die eigenen Programme sinnvoll zu erweitern.   Programmierung mit Python II   Aufbauend auf den Grundlagen aus den ersten Kurswochen kommen in den darauffolgenden Python-Bibliotheken zur Datenanalyse zum Einsatz. Diese dienen zum Beispiel der Datenakquise und der Datenaufbereitung. Sie werden bei der Datenakquise automatisiert Webseiten abfragen und bei der Datenaufbereitung den Umgang mit lückenhaften und inkonsistenten tabellarischen Daten üben. Einen Kern dieses Kursteils nimmt die Visualisierung ein. Diagramme unterschiedlicher Art werden Sie mit Python automatisiert erstellen. Darauf aufbauend werden Sie die Anwendung verschiedener uni-, bi- und multivariater statistischer Verfahren sowie die Erstellung von Interfenz- und Prädiktionsmodellen erlernen. Techniken des maschinellen Lernens bilden hierbei einen Schwerpunkt.   Programmierung mit Python III   Im dritten Teil des Kurses lernen Sie professionelle Entwicklungstools für Python kennen. Sie werden zunehmend Funktionen, Klassen und Module selbst entwickeln. Sie werden lernen große Datenmengen („big data“) effizient zu analysieren und zu visualisieren. Sie werden Methoden erlernen, die es Ihnen ermöglichen, Inhalte von Webseiten systematisch zu extrahieren und diese programmatisch zu analysieren. Die Erstellung einer Testumgebung und die Sicherung von Code mittels Versionskontrolle soll Ihnen helfen, besseren und robusteren Code zu entwickeln.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/clean_text.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the computer do the work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge 5: Write a function `extract_comments` that takes in links and returns the extracted text from all links. \n",
    "> * #### Make sure your function has an argument the provides the number of links to be followed.\n",
    "> * #### Also try to implement the `time.sleep` function with random sleeping time. This does make it more likely that your IP is not flagged ;-)\n",
    "\n",
    ">    `from numpy import random`   \n",
    ">    `import time`   \n",
    ">    `r = random.random()`   \n",
    ">    `time.sleep(r)`\n",
    "> * #### Consider reusing the functions `text_extraction` and `clean_text` from above. \n",
    "> * #### Make sure your function returns one string and no duplicates. The built-in functions `set` and `\" \".join` may be handy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/extract_comments.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_comments(links=df[\"course_links\"], n_links=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the wordcloud using stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('de') + get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(stopwords=stop_words).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a really fancy wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "mask = np.array(Image.open(\"../data/images/berlin_bear.png\"))   #choose mask\n",
    "\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stop_words,\n",
    "    background_color=\"white\",\n",
    "                    mask=mask,\n",
    "                    mode=\"RGB\",\n",
    "                    random_state=42\n",
    "                    ).generate(text)\n",
    "\n",
    "\n",
    "# Display the generated image:\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on PNG images\n",
    "\n",
    "PNG images may include transparency, but the wordcloud function expects the background to be white.\n",
    "This short code snippet will replace a transparent background with a white one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "mask = Image.open(\"../data/images/python.png\")   #choose mask\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(mask)[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the mask contains **4** channels - RGBA. \n",
    "\n",
    "The first three channels are the well know RGB (red, green, blue) values with the last one being the _Alpha_ value (transparency; 0 being completely transparent).\n",
    "\n",
    "See: [RGBA Color Model (Wikipedia)](https://en.wikipedia.org/wiki/RGBA_color_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = Image.new(\"RGBA\", mask.size, (255,255,255,255)) # create dummy background with white color only (255, 255, 255, no transparency)\n",
    "mask = Image.alpha_composite(background, mask) # overlay mask over background\n",
    "mask = np.array(mask)\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[0,0] # note that background is now white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stop_words,\n",
    "    background_color=\"white\",\n",
    "                    mask=mask,\n",
    "                    mode=\"RGB\",\n",
    "                    random_state=42\n",
    "                    ).generate(text)\n",
    "\n",
    "\n",
    "# Display the generated image:\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "ax.imshow(wordcloud)\n",
    "ax.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Challenge 6: Improve the wordcloud as you whish. \n",
    "> * #### Therfore you may play around with any arguments of the `wordcloud` function.\n",
    "> * #### Feel free to add any other mask of your choice. (watch out for transparent PNGs)\n",
    "> * #### Add or remove stopwords as you like.\n",
    "> * #### In order to have more fun the full data set is provided to you. Uncomment the cell below to access the extracted text from all currently available courses of ABV (full_text). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "full_text = pickle.load(open('../data/full_text.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=stop_words,\n",
    "    background_color=\"white\",\n",
    "                    mask=mask,\n",
    "                    mode=\"RGB\",\n",
    "                    random_state=42\n",
    "                    ).generate(full_text)\n",
    "\n",
    "\n",
    "# Display the generated image:\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "ax.imshow(wordcloud)\n",
    "ax.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
