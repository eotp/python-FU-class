{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installationsanweisungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "conda install -c conda-forge geopandas\n",
    "\n",
    "conda install -c conda-forge cartopy\n",
    "\n",
    "\n",
    "---------------------------------------\n",
    "## Win : pip install gdal\n",
    "\n",
    "## Mac : brew install gdal\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorative Data Analysis in Action: global power plant distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the subsequent notebooks we apply a variety of EDA techniques on a real world data set.\n",
    "\n",
    "The data set [__Global Power-Plants__](https://www.kaggle.com/ramjasmaurya/global-powerplants) is avaiable on [Kaggle](https://www.kaggle.com/).\n",
    "\n",
    "\n",
    "\n",
    "It was already downloaded for you and is found in the `datasets` folder:\n",
    "\n",
    "    ../data/powerplants.csv\n",
    "\n",
    "<img src=\"./_img/power_plant.webp\"> \n",
    "\n",
    "Source: [The Quint](https://www.thequint.com/news/environment/thermal-power-plants-use-up-more-water-than-permitted-rti-data-shows/).\n",
    "\n",
    "### Content\n",
    "\n",
    "This dataset consists of information about power plants worldwide. Each record includes the name, country, energy source type, geographic location, start date and other data elements. In this data analysis we want to learn someting about the geospatial distribution and the energy share of power plants in the world and in specific regions. Referring to the global goals of reducing the greenhouse gas emissions the energy production sector is one of the most important one. Actual knowledge about the specific energy share of green / fossil energy source and its change in time is therefore an important information for political decissions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import statements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [20,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./_img/Time_data_science.png)\n",
    "\n",
    "Source: [Gil Press (2016)](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#55852a146f63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pd.read_csv(\"../data/powerplants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "[Data cleansing or data cleaning](https://en.wikipedia.org/wiki/Data_cleansing) is the process of **detecting and correcting (or removing) corrupt or inaccurate records** from a record set, table, or database and refers to **identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with incomplete (`NaN`) and irrelevant data\n",
    "\n",
    "Missing values in data sets are a well-known problem as nearly everywhere, where data is measured and recorded, issues with missing values occur. Various reasons lead to missing values: values may not be measured, values may be measured but get lost or values may be measured but are considered unusable. Missing values can lead to problems, because often further data processing and analysis steps rely on complete data sets. Therefore missing values need to be replaced with reasonable values. In statistics this process is called **imputation**.\n",
    "\n",
    "When faced with the problem of missing values it is important to understand the mechanism that causes missing data. Such an understanding is useful, as it may be employed as background knowledge for selecting an appropriate imputation strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for `NaN`**\n",
    "\n",
    "Note that in many cases missing values are assigned special characters, such as `-999`, `NA`, `k.A.` etc.; hence, you as a data analyst are responsible for taking appropriate action.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Challenge:** Calculate the percentage of NaN-values in each column. (4 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/percentage.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strategies to deal with missing data in Python**\n",
    "\n",
    "In general there are many options to consider when imputing missing values, for example:\n",
    "* A constant value that has meaning within the domain, such as 0, distinct from all other values.\n",
    "* A value from another randomly selected record.\n",
    "* A mean, median or mode value for the column.\n",
    "* A value estimated by another predictive model.\n",
    "\n",
    "There are some libraries implementing more or less advanced missing value imputation strategies such as \n",
    "\n",
    "* [`statsmodels`](http://www.statsmodels.org/dev/imputation.html) ([Multiple Imputation with Chained Equations (MICE)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/))\n",
    "* [`fancyimpute`](https://github.com/iskandr/fancyimpute) (matrix completion and imputation algorithms)\n",
    "* [`scikit-learn`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html) (mean, median, most frequent)\n",
    "* [`pandas`](https://pandas.pydata.org/pandas-docs/stable/missing_data.html) ([`fillna`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html), [`interpolate`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.interpolate.html) methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our working strategy to deal with missing data**\n",
    "\n",
    "_Owing to the fact that the amount of missing values in our data set is considerable high and that we can not easily do predictions or assumptions for the specific missing values, we simply remove these columns with a high amount of missing data._ \n",
    "\n",
    "_Nethertheless we have to keep in mind that our dataset still consists of gaps. Therefore we have to handle these gaps individually in the following analysis._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Challenge:** Drop all columns in the dataframe that have more than 60 percent null values. Refactor your code afterwards and externalise your code into a function that takes the original dataframe and a threshold. The function should return the new dataframe.\n",
    "\n",
    "> **Hint**: Make use of the `df.index` attribute. (8 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: The index attribute allows us to extract the column names in this case\n",
    "ratios = ((pp.isnull().sum() / pp.shape[0]) * 100).round(2)\n",
    "display(ratios)\n",
    "# for example here are the columns for which the \"missing-ness\" ratio is 0\n",
    "print() # new line\n",
    "print(\"Columns with a ratio of 0\")\n",
    "display(ratios.loc[ratios == 0.0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint:* Remember the `copy()` method to return the result of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/filter_dataframe.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of columns before cleanup: {pp.shape[1]}\")\n",
    "pp = filter_dataframe(pp, threshold=60)\n",
    "print(f\"Number of columns after cleanup: {pp.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dealing with data structures\n",
    "\n",
    "Upon closer inspection we see that the `start date` column is stored as a floating point number only containing a year (which is rather an integer, isn't it?).\n",
    "\n",
    "For simplicity let's transform it to integer numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp[[\"country code\", \"start date\"]].sample(10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp['start date'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to transform the column to a integer datatype we have to deal with the `NaN` values.\n",
    "\n",
    "For example by assigning a distinct value that can't be achieved inside the column itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Challenge:** Assign an integer to NaN values using the `fillna()` and change the data type to integer using the `astype()` method. (5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/imputation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp[[\"country code\", \"start date\"]].sample(10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of spatial datasets\n",
    "\n",
    "_Note: In the subsequent cells we load Python library for spatial data analysis, such as `shapely`, `fiona`, `geopandas`, `cartopy` and `folium`. Make sure that you have installed the [GDAL bindings](http://www.gdal.org/index.html) on your computer._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we make use third party libraries for visualisation of the geospatial information, such as [GeoPandas](http://geopandas.org/index.html) and [shapely](http://toblerity.org/shapely/), which abstract away many algorithmic or computational issues related to spatial data processing and plotting by integrating the workhorses of geospatial computing, such as [GEOS](http://trac.osgeo.org/geos/), [GDAL](http://www.gdal.org/), [OGR](http://gdal.org/1.11/ogr/) and [proj.4](http://proj4.org/), among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geographical information is stored and given by the `latitude` and `longitude` column. We can use these information to localize each power plant in the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform the variables `Target Latitude` and `Target Longitude` to spatial coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "geometry = [Point(xy) for xy in zip(pp['longitude'], pp['latitude'])]\n",
    "geometry[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Point` class does not yield any useful information upon printing it. \n",
    "\n",
    "However it contains `x` and `y` attributes that map our longitudes and latitudes onto x-y coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_0 = geometry[0]\n",
    "print(point_0.x)\n",
    "print(point_0.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is nothing more than a slightly more efficient way of storing coordinates. \n",
    "\n",
    "And the library that will allow us to describe coordinates uses exactly this representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the GeoPandas to make a pandas `DataFrame` spatially aware.**\n",
    "\n",
    "[GeoPandas](http://geopandas.org/index.html) extends the datatypes used by pandas to allow spatial operations on geometric types. Geometric operations are performed by [shapely](https://shapely.readthedocs.io/en/stable/). GeoPandas further depends on [fiona](https://fiona.readthedocs.io/en/stable/) for file access and descartes and [matplotlib](https://matplotlib.org/) for plotting.\n",
    "\n",
    "It combines the capabilities of pandas and shapely, providing geospatial operations in pandas and a high-level interface to multiple geometries to shapely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "gdf = gpd.GeoDataFrame(pp, geometry=geometry)\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure that for every entry we have a valid spatial coordinates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Challenge:** Check the columns `longitude` and `latitude` for NaN-values. (4 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/long_lat.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign a spatial coordinate reference system (`crs`) to our GeoPandas object**\n",
    "\n",
    "In general the CRS may be defined in several ways, for example the CRS may be defined as [Well-known text (WKT)](https://en.wikipedia.org/wiki/Well-known_text) format, or [JSON](https://en.wikipedia.org/wiki/JSON) format, or [GML](https://en.wikipedia.org/wiki/Geography_Markup_Language) format, or in the [Proj4](https://en.wikipedia.org/wiki/PROJ.4) format, among many others.\n",
    "\n",
    "The Proj4 format is a generic, string-based description of a CRS. It defines projection types and parameter values for particular projections. For instance the Proj4 format string for the [European Terrestrial Reference System 1989 (ETRS89)](https://en.wikipedia.org/wiki/European_Terrestrial_Reference_System_1989) is:\n",
    "\n",
    "    +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no\\_defs\n",
    "\n",
    "With respect to the enormous amount of existing CRS the [International Association of Oil & Gas Producers (IOGP)](https://en.wikipedia.org/wiki/International_Association_of_Oil_%26_Gas_Producers), formerly known as **_European Petroleum Survey Group (EPSG)_**, built a collection of definitions for global, regional, national and local coordinate reference systems and coordinate transformations, the [EPSG Geodetic Parameter Dataset](http://www.epsg.org/). Within this collection each particular coordinate reference systems gets an unique integer identifier, commonly denoted as EPSG. For instance, the EPSG identifier for the the latest revision of the [World Geodetic System (WGS84)](https://en.wikipedia.org/wiki/World_Geodetic_System) is simply [4326](http://spatialreference.org/ref/epsg/4326/).\n",
    "\n",
    "\n",
    "A nice look up page for different coordinate reference systems is found [here](https://epsg.io/) and a fancy visualization of many prominent map projections is found [here](https://bl.ocks.org/mbostock/raw/3711652/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.set_crs('epsg:4326', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context matters: Load _Natural Earth countries_ dataset, bundled with GeoPandas\n",
    "\n",
    "[Natural Earth](http://www.naturalearthdata.com/) is a public domain map dataset available at 1:10m, 1:50m, and 1:110 million scales. Featuring tightly integrated vector and raster data, with Natural Earth you can make a variety of visually pleasing, well-crafted maps with cartography or GIS software. A subset comes bundled with GeoPandas and is accessible from the `gpd.datasets` module. We’ll use it as a helpful global base layer map.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.plot(facecolor='lightgray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine world map and the power plant data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = world.plot(facecolor='lightgray') # create base plot (world map)\n",
    "gdf.plot(\n",
    "    ax=base,         # specify object to plot over\n",
    "    marker='o',      # select shape of each plotted point (circle, cross, etc.)\n",
    "    color='red',     # color\n",
    "    markersize=2,    # size of each plotted point\n",
    "    alpha=0.1        # transparency of each plotted point (areas with less powerplants will become less visible)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Spatial Filtering the data by geolocation\n",
    "\n",
    "In a world view the points observations overlap each over a lot. So we can not see any pattern. For those purpose it is usefull, to set the focus of the research to a specific geographical region. In the following we want to focus on a continental and a country view.\n",
    "\n",
    "**Europe focus**\n",
    "\n",
    "As a first example we want to restrict our analysis to data points, which refer to an area within Europe. Although it is not straightforward to define Europe as an entity, in terms of geography, politics or sphere of cultural identity, we define Europe as an area between the coordinates  \n",
    "\n",
    "$$\\text{33.0 to 73.5°N and 27.0°W to 45.0°E.}$$\n",
    "\n",
    "In order to represent that area spatially; we construct a `Polygon` object to represent the [bounding box](https://en.wikipedia.org/wiki/Minimum_bounding_box) of Europe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "# generate geopandas object\n",
    "poly_europe = gpd.GeoSeries([Polygon([(-27,33), (45,33), (45,73.5), (-27,73.5)])])\n",
    "bb_europe = gpd.GeoDataFrame({'geometry': poly_europe})\n",
    "# assign crs\n",
    "bb_europe.set_crs(epsg=4326, inplace=True)\n",
    "bb_europe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot world map and bounding box of Europe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = world.plot(facecolor='lightgray')\n",
    "bb_europe.plot(ax=base, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subset (intersect) the GeoPandas `DataFrame` with the bounding box of Europe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_europe = gpd.sjoin(gdf, bb_europe, how=\"inner\", predicate='intersects').drop(\"index_right\", axis=1)\n",
    "\n",
    "print(gdf_europe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_europe.plot(markersize=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We catch a bit of the African and Asian continent**\n",
    "\n",
    "Fortunately the Naturalearth dataset also contains continent definitions. We can again subset these together to take the intersection of both definitions of Europe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_europe = gpd.overlay(gdf_europe, world.loc[world['continent'] == 'Europe'], how='intersection')\n",
    "print(gdf_europe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_europe.plot(markersize=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result again is not perfect as we don't include the european part of Turkey for example.\n",
    "However the Naturalearth dataset alone also counts Russia and some islands far away from the European continent as Europe.\n",
    "\n",
    "This definition of Europe should be sufficient for our analysis. Although feel free to bring in your own if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.loc[world['continent'] == 'Europe'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order to keep the spatial context we extract the area of Europe from the world map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europe = gpd.overlay(world, bb_europe, how='intersection')\n",
    "europe.set_crs('epsg:4326', inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = europe.plot(facecolor='lightgray')\n",
    "gdf_europe.plot(ax=base, marker='o', markersize=2, alpha=0.75);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with categorical data\n",
    "\n",
    "`geopandas` includes `column`, `categorical` and `legend` keywords in order to better distinguish between individual types of data inside the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = europe.plot(facecolor='lightgray')\n",
    "gdf_europe.plot(ax=base, marker='o', markersize=2, alpha=0.75, column='primary_fuel', categorical=True, legend=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing individual sub-categories\n",
    "\n",
    "As our dataset provides many categories the resulting plot is not necessarily ideal.\n",
    "\n",
    "Way too many categories are displayed and some even share the same color.\n",
    "\n",
    "One way to circumvent this is to chose individual categories we want to display.\n",
    "For example only the most prominent in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_europe[\"primary_fuel\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only get top-5 categories\n",
    "gdf_europe[\"primary_fuel\"].value_counts().iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_fuels = gdf_europe[\"primary_fuel\"].value_counts().iloc[:5].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_fuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = europe.plot(facecolor='lightgray')\n",
    "gdf_europe.loc[gdf_europe[\"primary_fuel\"].isin(top_5_fuels)].plot(\n",
    "        ax=base,                # plotting object to plot over\n",
    "        marker='o',             # marker shape\n",
    "        markersize=2,           # marker size\n",
    "        alpha=0.75,             # transparency\n",
    "        column='primary_fuel',  # column to pick categories from\n",
    "        categorical=True,       # force the plot to chose different colors for individual categories\n",
    "        legend=True             # include legend\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Colors this way are selected by GeoPandas and may be unfitting. For better control we can loop over each fuel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary containing colors for each top_5 fuel kind\n",
    "colors = {\n",
    "    \"Solar\": \"yellow\",\n",
    "    \"Wind\": \"blue\",\n",
    "    \"Hydro\": \"aqua\",\n",
    "    \"Gas\": \"red\",\n",
    "    \"Biomass\": \"darkgoldenrod\",\n",
    "}\n",
    "\n",
    "# Plot base map\n",
    "base = europe.plot(facecolor='lightgray')\n",
    "for fuel in top_5_fuels:\n",
    "    # Plot each respective fuel\n",
    "    gdf_europe.loc[gdf_europe[\"primary_fuel\"] == fuel].plot(\n",
    "        ax=base,                # plotting object to plot over\n",
    "        marker='o',             # marker shape\n",
    "        markersize=2,           # marker size\n",
    "        alpha=0.5,              # transparency\n",
    "        color = colors[fuel],   # color\n",
    "        label = fuel            # name in legend\n",
    ")           \n",
    "base.legend() # show legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Still pretty ugly although more informative than the original plot**\n",
    "\n",
    "For further analysis let's add a new column which decides whether the fuel type is \"green\", i.e. sustainable, or not.\n",
    "\n",
    "**Feel free to add further categories or change the definition of sustainable energy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['primary_fuel'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_fuels = ['Hydro', 'Solar', 'Wind', 'Biomass', 'Wave and Tidal', 'Geothermal']\n",
    "def is_green(entry):\n",
    "    return entry in green_fuels\n",
    "\n",
    "gdf['green'] = gdf['primary_fuel'].apply(is_green)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Challenge:** Try to select only entries within `green_fuels` using the `loc` and `isin()` method. (5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/locisin.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = world.plot(facecolor='lightgray')\n",
    "\n",
    "gdf.loc[gdf['green'] == True].plot(ax = base, markersize=2, alpha=0.1, color='green', label='green')\n",
    "gdf.loc[gdf['green'] == False].plot(ax = base, markersize=2, alpha=0.1, color='red', label='unsustainable')\n",
    "base.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The legend directly infers the marker shape, label and color from the plot. Since the low alpha we can barely see the corresponding markers.**\n",
    "> Hack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = world.plot(facecolor='lightgray')\n",
    "\n",
    "# Add dummy plots (of empty lists -> nothing to be plotted) for correct legend assignment\n",
    "base.scatter([],[],color='green', marker='o', label='green')\n",
    "base.scatter([],[],color='red', marker='o', label='unsustainable')\n",
    "\n",
    "gdf.loc[gdf['green'] == True].plot(ax = base, markersize=2, alpha=0.1, color='green')\n",
    "gdf.loc[gdf['green'] == False].plot(ax = base, markersize=2, alpha=0.1, color='red')\n",
    "base.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regional focus - Germany**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = world[world.name == \"Germany\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_germany = gdf.loc[gdf['country'] == \"Germany\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "germany = gpd.overlay(world, filtered, how='intersection')\n",
    "germany.set_crs('epsg:4326', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = germany.plot(facecolor='lightgray')\n",
    "# Add dummy plots (of empty lists -> nothing to be plotted) for correct legend assignment\n",
    "base.scatter([],[],color='green', marker='o', label='green')\n",
    "base.scatter([],[],color='red', marker='o', label='unsustainable')\n",
    "\n",
    "gdf_germany.loc[gdf_germany['green'] == True].plot(ax = base, markersize=4, alpha=0.7, color='green')\n",
    "gdf_germany.loc[gdf_germany['green'] == False].plot(ax = base, markersize=4, alpha=0.7, color='red')\n",
    "base.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable size of plotted points\n",
    "\n",
    "We can also add a column name as `markersize` argument inside the plotting method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = germany.plot(facecolor='lightgray')\n",
    "# Add dummy plots (of empty lists -> nothing to be plotted) for correct legend assignment\n",
    "base.scatter([],[],color='green', marker='o', label='green')\n",
    "base.scatter([],[],color='red', marker='o', label='unsustainable')\n",
    "\n",
    "gdf_germany.loc[gdf_germany['green'] == True].plot(ax = base, markersize=\"estimated_generation_gwh_2020\", alpha=0.7, color='green')\n",
    "gdf_germany.loc[gdf_germany['green'] == False].plot(ax = base, markersize=\"estimated_generation_gwh_2020\", alpha=0.7, color='red')\n",
    "base.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalizing the values**\n",
    "\n",
    "The resulting values are way too big. Before we have set `markersize = 4` and now the values of the column are taken which can be far greater than that.\n",
    "\n",
    "One way to deal with that, is to scale the data beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E.g. Scale data to a set upper bound $m \\cdot \\frac{x}{max(x)}$**\n",
    "\n",
    "The maximum value in $x$ will now be equal to $m$ and all other values will keep the same relative distance to it.\n",
    "\n",
    "This may result in many very small points depending on the distribution of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound = 400\n",
    "gdf_germany['estimated_generation_gwh_2020_scaled'] = upper_bound * gdf_germany['estimated_generation_gwh_2020'] / gdf_germany['estimated_generation_gwh_2020'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_germany['estimated_generation_gwh_2020_scaled'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = germany.plot(facecolor='lightgray')\n",
    "# Add dummy plots (of empty lists -> nothing to be plotted) for correct legend assignment\n",
    "base.scatter([],[],color='green', marker='o', label='green')\n",
    "base.scatter([],[],color='red', marker='o', label='unsustainable')\n",
    "\n",
    "gdf_germany.loc[gdf_germany['green'] == True].plot(ax = base, markersize=\"estimated_generation_gwh_2020_scaled\", alpha=0.7, color='green')\n",
    "gdf_germany.loc[gdf_germany['green'] == False].plot(ax = base, markersize=\"estimated_generation_gwh_2020_scaled\", alpha=0.7, color='red')\n",
    "base.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E.g. Min-Max Scaling between $[a,b]$: $a + \\frac{(x-min(x)) \\cdot (b - a)}{max(x) - min(x)}$**\n",
    "\n",
    "This way we can control both the minimum and maximum values in which our values will end up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Challenge:** Try to calculate Min-Max Scaling for the column `gdf_germany['estimated_generation_gwh_2020']` with `a=4` and `b=400`. (10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/_solutions/minmax_manually.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the function `minmax_scaler()` from the `helper` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import minmax_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_germany['estimated_generation_gwh_2020_scaled'] = minmax_scaler(gdf_germany['estimated_generation_gwh_2020'], lower_bound=4, upper_bound=400)\n",
    "gdf_germany['estimated_generation_gwh_2020_scaled'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = germany.plot(facecolor='lightgray')\n",
    "# Add dummy plots (of empty lists -> nothing to be plotted) for correct legend assignment\n",
    "base.scatter([],[],color='green', marker='o', label='green')\n",
    "base.scatter([],[],color='red', marker='o', label='unsustainable')\n",
    "\n",
    "gdf_germany.loc[gdf_germany['green'] == True].plot(ax = base, markersize=\"estimated_generation_gwh_2020_scaled\", alpha=0.7, color='green')\n",
    "gdf_germany.loc[gdf_germany['green'] == False].plot(ax = base, markersize=\"estimated_generation_gwh_2020_scaled\", alpha=0.7, color='red')\n",
    "base.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the \"big\" dots stayed somewhat similar whereas the smaller ones grew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Even more colorful plots\n",
    "\n",
    "We offer a function to produce plots using earth projections you may be more familiar with.\n",
    "\n",
    "Feel free to take a look at it, the source file can be found in `src/utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import cuteplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?cuteplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuteplot(gdf_europe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuteplot(gdf_europe.loc[gdf_europe[\"primary_fuel\"] == \"Solar\"], color = \"blue\", alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuteplot(gdf_europe.loc[gdf_europe[\"primary_fuel\"] == \"Solar\"], color = \"blue\", alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The function returns a `Figure` and `Axes` object. Allowing for further manipulation. For example setting a title as shown here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = cuteplot(gdf.loc[gdf[\"primary_fuel\"] == \"Hydro\"], color = \"blue\", alpha = 0.3, map_extent=None, label = \"Solar\")\n",
    "\n",
    "ax.set_title(\"Hydro Power Plants across the Earth\", size=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can also pass the ax object back inside and plot further points on top**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuteplot(gdf.loc[gdf[\"primary_fuel\"] == \"Coal\"], color = \"red\", alpha = 0.3, map_extent=None, label = \"Coal\", ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.legend(fontsize=16)\n",
    "ax.set_title(\"Hydro and Coal Powerplants across the earth\", size=24)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving our Dataframe to disk for further analysis\n",
    "\n",
    "We spend quite a lot of time for prepating our dataset for our analysis. At the end we want to save our dataframe to disk so that we can load this work state at every time again. There exist plenty of options, in this tutorial we want to use the `pickle` library to serialize our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_world = gpd.overlay(gdf, world, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_world = gdf_world[['country code', 'country', 'name of powerplant', 'capacity in MW',\n",
    "       'latitude', 'longitude', 'primary_fuel', 'start date', 'owner of plant',\n",
    "       'geolocation_source', 'estimated_generation_gwh_2020', 'green', 'continent', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_europe = gpd.sjoin(gdf_world.loc[gdf_world['continent'] == \"Europe\"], bb_europe, how=\"inner\", predicate='intersects').drop(\"index_right\", axis=1)\n",
    "gdf_germany = gdf_world.loc[gdf_world['country'] == \"Germany\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment to serialize the data to disk\n",
    "import pickle\n",
    "pickle.dump(gdf_world, open(\"../data/gdf_world.p\", \"wb\"))\n",
    "pickle.dump(gdf_europe, open(\"../data/gdf_europe.p\", \"wb\"))\n",
    "pickle.dump(gdf_germany, open(\"../data/gdf_germany.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## alternatively save in geojson format\n",
    "gdf_world.to_file(\"../data/gdf_world.geojson\", driver='GeoJSON')\n",
    "gdf_europe.to_file(\"../data/gdf_europe.geojson\", driver='GeoJSON')\n",
    "gdf_germany.to_file(\"../data/gdf_germany.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### gdf = gpd.read_file(\"file.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready!! Now it's your turn :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
